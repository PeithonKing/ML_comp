{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d06e772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "#Entire Notebook \n",
    "#There are net 6 models in our ML PipeLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f85583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f59da0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8340/1804596755.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"HIGGS.CSV\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdata_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlabel_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdata_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_set\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdata_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4d6231",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966b8f35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe3394e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb09f39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8704afa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cln():    \n",
    "    df=pd.read_csv(\"HIGGS.CSV\",header=None)\n",
    "    data_set=np.array(df.iloc[:,1:])\n",
    "    label_set=np.array(df.iloc[:,0])\n",
    "    data_set=np.round(data_set,5)\n",
    "    data_set=data_set.astype(np.float32)\n",
    "    label_set=label_set.astype(np.int32)\n",
    "    df_Data_set=pd.DataFrame(data_set)\n",
    "    df_Label_set=pd.DataFrame(label_set)\n",
    "    normed_df_Data_set = ((df_Data_set - df_Data_set.min()) / (df_Data_set.max() - df_Data_set.min())).astype(np.float32)\n",
    "    df_normed_Data_set=pd.DataFrame(np.array(normed_df_Data_set.iloc[:,1:]))\n",
    "    df_normed_Data_set.to_csv(\"DataSetNormalised.csv\")\n",
    "    #------------------------------------------------------------\n",
    "    df_data=pd.read_csv(\"DataSetNormalised.csv\")\n",
    "    df_label=pd.read_csv(\"LabelSet.csv\")\n",
    "    x=np.array(df_data.iloc[:,1:])\n",
    "    y=np.array(df_label.iloc[:,1])\n",
    "    x=x.astype(np.float32)\n",
    "    x_train=x[:][:10000000]\n",
    "    y_train=y[:][:10000000]\n",
    "    x_test=x[:][10000000:]\n",
    "    y_test=y[:][10000000:]\n",
    "#--------------------------------------------------------------\n",
    "def DNN5_fc():\n",
    "    \n",
    "#Deep Neural Network DNN5\n",
    "    model=tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(300,input_shape=(28,),activation='swish'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(300,activation='swish'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(300,activation='swish'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(300,activation='swish'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(300,activation='swish'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "        learning_rate=0.01,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy','AUC']\n",
    "        )\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        epochs=1,\n",
    "        batch_size=10240,\n",
    "        validation_data=(x_test,y_test)\n",
    "    )\n",
    "    model.save('DNN_5')\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "def RN52_fc():\n",
    "    d_rate=0.1 #The rate of droupout for the layers in the ResNet\n",
    "    act='gelu'#The activation function use ('relu','swish','elu','gelu','selu')\n",
    "    lr=0.01 #The default learning rate Default 0.01\n",
    "    mom=0.9 #The momentum parameter Default 0.9\n",
    "    test_split=1 #Default 1 and range[1,0)\n",
    "    #Reducing test_split will make the model faster but will also reduce the accuracy of the Validation metric\n",
    "    #On changing test_split run the lower initialisation cell, marked below**\n",
    "    \n",
    "    def identity_block(X,no_nodes,Activation):#A custom Residual identity block with a skip connection\n",
    "        X_shortcut=X\n",
    "        #There will be 3 layers in total for the sequential connection\n",
    "        #Activation will be specified by the user as per the Keras Documentation\n",
    "        #no_nodes denote the size of the model which will be built\n",
    "        #Applying Batch Normalisation to size the inputs\n",
    "        #Layer 1 -----------------------------------------------\n",
    "        X=tf.keras.layers.Dense(no_nodes,activation=Activation)(X)\n",
    "        X=tf.keras.layers.BatchNormalization(axis=1)(X)\n",
    "        X=tf.keras.layers.Dropout(d_rate)(X)\n",
    "        #--------------------------------------------------------\n",
    "        #Layer 2-------------------------------------------------\n",
    "        X=tf.keras.layers.Dense(no_nodes,activation=Activation)(X)\n",
    "        X=tf.keras.layers.BatchNormalization(axis=1)(X)\n",
    "        X=tf.keras.layers.Dropout(d_rate)(X)\n",
    "        #--------------------------------------------------------\n",
    "        #Layer 3-------------------------------------------------\n",
    "        X=tf.keras.layers.Dense(no_nodes,activation=Activation)(X)\n",
    "        X=tf.keras.layers.Dropout(d_rate)(X)\n",
    "        #--------------------------------------------------------\n",
    "        #Layer 4-------------------------------------------------\n",
    "        X= tf.keras.layers.Add()([X_shortcut, X])\n",
    "        X=tf.keras.layers.Activation('relu')(X)\n",
    "        #--------------------------------------------------------\n",
    "        X_shortcut2=X\n",
    "         #Layer 5 -----------------------------------------------\n",
    "        X=tf.keras.layers.Dense(no_nodes,activation=Activation)(X)\n",
    "        X=tf.keras.layers.BatchNormalization(axis=1)(X)\n",
    "        X=tf.keras.layers.Dropout(d_rate)(X)\n",
    "        #--------------------------------------------------------\n",
    "        #Layer 6-------------------------------------------------\n",
    "        X=tf.keras.layers.Dense(no_nodes,activation=Activation)(X)\n",
    "        X=tf.keras.layers.BatchNormalization(axis=1)(X)\n",
    "        X=tf.keras.layers.Dropout(d_rate)(X)\n",
    "        #--------------------------------------------------------\n",
    "        #Layer 7-------------------------------------------------\n",
    "        X=tf.keras.layers.Dense(no_nodes,activation=Activation)(X)\n",
    "        X=tf.keras.layers.Dropout(d_rate)(X)\n",
    "        #--------------------------------------------------------\n",
    "        X= tf.keras.layers.Add()([X_shortcut, X,X_shortcut2])\n",
    "        #Layer 8-------------------------------------------------\n",
    "        X=tf.keras.layers.Activation('relu')(X)\n",
    "        return X\n",
    "    inputs=tf.keras.layers.Input(shape=(28,))                    #Keras input layer\n",
    "    X=tf.keras.layers.Dense(300,activation=act)(inputs)\n",
    "    X=identity_block(X=X,no_nodes=300,Activation=act)\n",
    "    X=identity_block(X=X,no_nodes=300,Activation=act)\n",
    "    X=identity_block(X=X,no_nodes=300,Activation=act)\n",
    "    X=identity_block(X=X,no_nodes=300,Activation=act)\n",
    "    X=identity_block(X=X,no_nodes=300,Activation=act)        \n",
    "    outputs=tf.keras.layers.Dense(1,activation='sigmoid')(X)     #keras output layer\n",
    "    model=tf.keras.Model(inputs,outputs)#Model declaration\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "        learning_rate=0.01,\n",
    "        beta_1=mom,\n",
    "        beta_2=0.999),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy','AUC']\n",
    "    )\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\",\n",
    "                                                 factor=0.5,\n",
    "                                                 patience=5,\n",
    "                                                 min_lr=10**-6,\n",
    "                                                 cooldown=5,\n",
    "                                                 verbose = True)\n",
    "\n",
    "    history = History()\n",
    "\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        epochs=100,\n",
    "        batch_size=20000,\n",
    "        validation_data=(x_test,y_test),\n",
    "        callbacks=[cp_callback, reduce_lr, history]\n",
    "    )\n",
    "    model.save('ResNet_52')\n",
    "\n",
    "    with open(\"sunnyMamoni.txt\", \"w\") as f:\n",
    "    f.write(str(history.history))\n",
    "#-----------------------------------------------------------------------------------    \n",
    "def XGB_fc():\n",
    "        \n",
    "    model = XGBClassifier(tree_method='gpu_hist',\n",
    "                          booster = 'gbtree',\n",
    "                          max_depth = 14,\n",
    "                          n_estimators = 300,\n",
    "                          learning_rate = 0.4,\n",
    "                          reg_lambda=600\n",
    "                         ) # booster = gbtree, dart\n",
    "\n",
    "    eval_set = [(x_test_alpha, y_test_alpha)]\n",
    "\n",
    "    t0 = time()\n",
    "    a = model.fit(x_train_alpha,\n",
    "              y_train_alpha,\n",
    "              eval_metric=\"error\",\n",
    "              eval_set=eval_set,\n",
    "              verbose=True)\n",
    "    taken = time()-t0\n",
    "\n",
    "    y_test_alpha_pred = model.predict(x_test_alpha,ntree_limit=model.best_ntree_limit)\n",
    "    accuracy_test = accuracy_score(y_test_alpha, y_test_alpha_pred)\n",
    "    print(\"The testing accuracy\", accuracy_test*100)\n",
    "\n",
    "    y_train_alpha_pred=model.predict(x_train_alpha,ntree_limit=model.best_ntree_limit)\n",
    "    accuracy_train = accuracy_score(y_train_alpha, y_train_alpha_pred)\n",
    "    print(\"The training accuracy\", accuracy_train*100)\n",
    "    model.save_model(\"XGBoost.json\")\n",
    "#------------------------------------------------------\n",
    "def S1024_fc():\n",
    "\n",
    "    model=tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(1024,input_shape=(28,),activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(1024,activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(128,activation='swish'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "        ])\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "        learning_rate=0.0001,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy','AUC']\n",
    "        )\n",
    "    model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    batch_size=10240,\n",
    "    validation_data=(x_test,y_test),\n",
    "    callbacks=[cp_callback]\n",
    "        )\n",
    "    model.save('NN_Shallow_1024')\n",
    "#-------------------------------------------------------------------\n",
    "def S2048_fc():\n",
    "    model=tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(2048,input_shape=(28,),activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(2048,activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "        ])\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "        learning_rate=0.01,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy','AUC']\n",
    "        )\n",
    "    model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=1,\n",
    "    batch_size=10240,\n",
    "    validation_data=(x_test,y_test),\n",
    "    callbacks=[cp_callback]\n",
    "        )\n",
    "    model.save('NN_Shallow_2048')\n",
    "#---------------------------------------------------------------------------\n",
    "def S10240_fc():\n",
    "    model=tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10240,input_shape=(28,),activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(\n",
    "        learning_rate=0.001,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy','AUC']\n",
    "    )\n",
    "    model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    batch_size=10240,\n",
    "    callbacks=[cp_callback]\n",
    "    )\n",
    "    model.save('NN_Shallow_2048')\n",
    "#----------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddf0f973",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_8340/1072973920.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\ML_RTC\\AppData\\Local\\Temp/ipykernel_8340/1072973920.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    def ens_fc(x_test)\u001b[0m\n\u001b[1;37m                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def ens_fc(x_test)    \n",
    "    model_xgb=XGBClassifier()\n",
    "    model_xgb.load_model(\"XGBoost.json\")\n",
    "    y_test_pred_xgb=model_xgb.predict_proba(x_test)\n",
    "    y_pred_xgb=y_test_pred_xgb[:,1]\n",
    "    y_test_pred_xgb=np.reshape(y_pred_xgb,(1000000,1))\n",
    "    model_DNN_5 = tf.keras.models.load_model('DNN_5')\n",
    "    y_test_pred_DNN_5=model_DNN_5.predict(x_test)\n",
    "    model_NN_Shallow_1024 = tf.keras.models.load_model('NN_Shallow_1024')\n",
    "    y_test_pred_NN_Shallow_1024=model_NN_Shallow_1024.predict(x_test)\n",
    "    model_NN_Shallow_2048 = tf.keras.models.load_model('NN_Shallow_2048')\n",
    "    y_test_pred_NN_Shallow_2048=model_NN_Shallow_2048.predict(x_test)\n",
    "    model_NN_Shallow_10240 = tf.keras.models.load_model('NN_Shallow_10240')\n",
    "    y_test_pred_NN_Shallow_10240=model_NN_Shallow_10240.predict(x_test)\n",
    "    model_ResNet_52 = tf.keras.models.load_model('ResNet_52')\n",
    "    y_test_pred_model_ResNet_52=model_ResNet_52.predict(x_test)\n",
    "\n",
    "    \"\"\"\n",
    "    for i in range(10000):\n",
    "    pre=(np.random.rand(6,1))\n",
    "    y_pred_ensemble=y_test_pred_DNN_5*pre[0]\n",
    "    y_pred_ensemble=y_pred_ensemble+y_test_pred_model_ResNet_52*pre[1]\n",
    "    y_pred_ensemble=y_pred_ensemble+y_test_pred_NN_Shallow_1024*pre[2]\n",
    "    y_pred_ensemble=y_pred_ensemble+y_test_pred_NN_Shallow_10240*pre[3]\n",
    "    y_pred_ensemble=y_pred_ensemble+y_test_pred_NN_Shallow_2048*pre[4]\n",
    "    y_pred_ensemble=y_pred_ensemble+y_test_pred_xgb*pre[5]\n",
    "    y_pred_ensemble=y_pred_ensemble/(pre[0]+pre[1]+pre[2]+pre[3]+pre[4]+pre[5])\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_ensemble)\n",
    "    auc=metrics.auc(fpr, tpr)\n",
    "    if auc>auc_max:\n",
    "        auc_max=auc\n",
    "        print(\"New Max detected\")\n",
    "        print(\"Auc\",auc)\n",
    "        print(\"--------------------------------------------\")\n",
    "        pre_max=pre\n",
    "    if i%1000==0:\n",
    "        print(str(i))\n",
    "    The above code is used to find the fine-tuned values for ensembling all the models\n",
    "    \"\"\"\n",
    "    pre=(0.9874,0.5109,0.4046,0.00178,0.18467,0.17939)\n",
    "    y_pred_ensemble=y_test_pred_DNN_5*pre[0]\n",
    "    y_pred_ensemble=y_pred_ensemble+y_test_pred_model_ResNet_52*pre[1]\n",
    "    y_pred_ensemble=y_pred_ensemble+y_test_pred_NN_Shallow_1024*pre[2]\n",
    "    y_pred_ensemble=y_pred_ensemble+y_test_pred_NN_Shallow_10240*pre[3]\n",
    "    y_pred_ensemble=y_pred_ensemble+y_test_pred_NN_Shallow_2048*pre[4]\n",
    "    y_pred_ensemble=y_pred_ensemble+y_test_pred_xgb*pre[5]\n",
    "    y_pred_ensemble=y_pred_ensemble/(pre[0]+pre[1]+pre[2]+pre[3]+pre[4]+pre[5])\n",
    "    #ntgt() Used for fine tuning of model, please dont uncomment it\n",
    "    return y_pred_ensemble    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036f55ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077e73da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
