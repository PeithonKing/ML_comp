{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb4a711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tensorflow.keras\n",
    "import os\n",
    "checkpoint_path = \"training_ResNet_52/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c1d9c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data=pd.read_csv(\"DataSetNormalised.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6357627f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.050286</td>\n",
       "      <td>0.369601</td>\n",
       "      <td>0.564642</td>\n",
       "      <td>0.021253</td>\n",
       "      <td>0.302234</td>\n",
       "      <td>0.062910</td>\n",
       "      <td>0.458154</td>\n",
       "      <td>0.186402</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.497833</td>\n",
       "      <td>0.486773</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.031874</td>\n",
       "      <td>0.038707</td>\n",
       "      <td>0.113156</td>\n",
       "      <td>0.055765</td>\n",
       "      <td>0.038036</td>\n",
       "      <td>0.061924</td>\n",
       "      <td>0.067874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.053521</td>\n",
       "      <td>0.567601</td>\n",
       "      <td>0.603004</td>\n",
       "      <td>0.097277</td>\n",
       "      <td>0.410338</td>\n",
       "      <td>0.097729</td>\n",
       "      <td>0.406137</td>\n",
       "      <td>0.043934</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.271923</td>\n",
       "      <td>0.499667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005662</td>\n",
       "      <td>0.031445</td>\n",
       "      <td>0.114120</td>\n",
       "      <td>0.059877</td>\n",
       "      <td>0.041314</td>\n",
       "      <td>0.062247</td>\n",
       "      <td>0.058134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.044327</td>\n",
       "      <td>0.802000</td>\n",
       "      <td>0.030564</td>\n",
       "      <td>0.029457</td>\n",
       "      <td>0.622152</td>\n",
       "      <td>0.098682</td>\n",
       "      <td>0.715906</td>\n",
       "      <td>0.896692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.725909</td>\n",
       "      <td>0.758206</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020806</td>\n",
       "      <td>0.045090</td>\n",
       "      <td>0.114118</td>\n",
       "      <td>0.057983</td>\n",
       "      <td>0.042641</td>\n",
       "      <td>0.050959</td>\n",
       "      <td>0.055869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.090465</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.768391</td>\n",
       "      <td>0.129367</td>\n",
       "      <td>0.753152</td>\n",
       "      <td>0.168172</td>\n",
       "      <td>0.222741</td>\n",
       "      <td>0.229380</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.364120</td>\n",
       "      <td>0.109674</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021726</td>\n",
       "      <td>0.041143</td>\n",
       "      <td>0.115758</td>\n",
       "      <td>0.042198</td>\n",
       "      <td>0.046364</td>\n",
       "      <td>0.065316</td>\n",
       "      <td>0.077971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.070221</td>\n",
       "      <td>0.566001</td>\n",
       "      <td>0.936645</td>\n",
       "      <td>0.057322</td>\n",
       "      <td>0.154448</td>\n",
       "      <td>0.055491</td>\n",
       "      <td>0.319775</td>\n",
       "      <td>0.235269</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425140</td>\n",
       "      <td>0.532329</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016970</td>\n",
       "      <td>0.057617</td>\n",
       "      <td>0.114235</td>\n",
       "      <td>0.049968</td>\n",
       "      <td>0.061272</td>\n",
       "      <td>0.051523</td>\n",
       "      <td>0.059396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999995</th>\n",
       "      <td>10999995</td>\n",
       "      <td>0.074864</td>\n",
       "      <td>0.708200</td>\n",
       "      <td>0.531054</td>\n",
       "      <td>0.097118</td>\n",
       "      <td>0.345951</td>\n",
       "      <td>0.224923</td>\n",
       "      <td>0.358620</td>\n",
       "      <td>0.879182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.480493</td>\n",
       "      <td>0.841457</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.018622</td>\n",
       "      <td>0.028159</td>\n",
       "      <td>0.116204</td>\n",
       "      <td>0.065760</td>\n",
       "      <td>0.044546</td>\n",
       "      <td>0.050472</td>\n",
       "      <td>0.054919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999996</th>\n",
       "      <td>10999996</td>\n",
       "      <td>0.029067</td>\n",
       "      <td>0.292001</td>\n",
       "      <td>0.818375</td>\n",
       "      <td>0.061103</td>\n",
       "      <td>0.391357</td>\n",
       "      <td>0.088460</td>\n",
       "      <td>0.558688</td>\n",
       "      <td>0.017989</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.456486</td>\n",
       "      <td>0.800867</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.018739</td>\n",
       "      <td>0.039215</td>\n",
       "      <td>0.119606</td>\n",
       "      <td>0.075558</td>\n",
       "      <td>0.047622</td>\n",
       "      <td>0.057437</td>\n",
       "      <td>0.066455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999997</th>\n",
       "      <td>10999997</td>\n",
       "      <td>0.036016</td>\n",
       "      <td>0.659000</td>\n",
       "      <td>0.936008</td>\n",
       "      <td>0.055004</td>\n",
       "      <td>0.560670</td>\n",
       "      <td>0.097729</td>\n",
       "      <td>0.508838</td>\n",
       "      <td>0.507019</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.817271</td>\n",
       "      <td>0.991564</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006538</td>\n",
       "      <td>0.032048</td>\n",
       "      <td>0.114361</td>\n",
       "      <td>0.053177</td>\n",
       "      <td>0.103900</td>\n",
       "      <td>0.076656</td>\n",
       "      <td>0.074661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999998</th>\n",
       "      <td>10999998</td>\n",
       "      <td>0.076397</td>\n",
       "      <td>0.524201</td>\n",
       "      <td>0.133552</td>\n",
       "      <td>0.121080</td>\n",
       "      <td>0.332522</td>\n",
       "      <td>0.087833</td>\n",
       "      <td>0.287096</td>\n",
       "      <td>0.866448</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.780094</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019038</td>\n",
       "      <td>0.033915</td>\n",
       "      <td>0.141387</td>\n",
       "      <td>0.057126</td>\n",
       "      <td>0.016474</td>\n",
       "      <td>0.041420</td>\n",
       "      <td>0.048286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999999</th>\n",
       "      <td>10999999</td>\n",
       "      <td>0.016050</td>\n",
       "      <td>0.430800</td>\n",
       "      <td>0.565597</td>\n",
       "      <td>0.061985</td>\n",
       "      <td>0.251052</td>\n",
       "      <td>0.029838</td>\n",
       "      <td>0.454319</td>\n",
       "      <td>0.140397</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169056</td>\n",
       "      <td>0.331733</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016887</td>\n",
       "      <td>0.026869</td>\n",
       "      <td>0.114274</td>\n",
       "      <td>0.037645</td>\n",
       "      <td>0.029818</td>\n",
       "      <td>0.021990</td>\n",
       "      <td>0.023210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11000000 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Unnamed: 0         0         1         2         3         4  \\\n",
       "0                  0  0.050286  0.369601  0.564642  0.021253  0.302234   \n",
       "1                  1  0.053521  0.567601  0.603004  0.097277  0.410338   \n",
       "2                  2  0.044327  0.802000  0.030564  0.029457  0.622152   \n",
       "3                  3  0.090465  0.320000  0.768391  0.129367  0.753152   \n",
       "4                  4  0.070221  0.566001  0.936645  0.057322  0.154448   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "10999995    10999995  0.074864  0.708200  0.531054  0.097118  0.345951   \n",
       "10999996    10999996  0.029067  0.292001  0.818375  0.061103  0.391357   \n",
       "10999997    10999997  0.036016  0.659000  0.936008  0.055004  0.560670   \n",
       "10999998    10999998  0.076397  0.524201  0.133552  0.121080  0.332522   \n",
       "10999999    10999999  0.016050  0.430800  0.565597  0.061985  0.251052   \n",
       "\n",
       "                 5         6         7    8  ...        18        19   20  \\\n",
       "0         0.062910  0.458154  0.186402  0.0  ...  0.497833  0.486773  1.0   \n",
       "1         0.097729  0.406137  0.043934  1.0  ...  0.271923  0.499667  0.0   \n",
       "2         0.098682  0.715906  0.896692  0.0  ...  0.725909  0.758206  0.0   \n",
       "3         0.168172  0.222741  0.229380  0.0  ...  0.364120  0.109674  0.0   \n",
       "4         0.055491  0.319775  0.235269  0.0  ...  0.425140  0.532329  0.0   \n",
       "...            ...       ...       ...  ...  ...       ...       ...  ...   \n",
       "10999995  0.224923  0.358620  0.879182  0.0  ...  0.480493  0.841457  1.0   \n",
       "10999996  0.088460  0.558688  0.017989  1.0  ...  0.456486  0.800867  1.0   \n",
       "10999997  0.097729  0.508838  0.507019  1.0  ...  0.817271  0.991564  0.0   \n",
       "10999998  0.087833  0.287096  0.866448  0.0  ...  0.780094  0.123205  0.0   \n",
       "10999999  0.029838  0.454319  0.140397  1.0  ...  0.169056  0.331733  0.0   \n",
       "\n",
       "                21        22        23        24        25        26        27  \n",
       "0         0.031874  0.038707  0.113156  0.055765  0.038036  0.061924  0.067874  \n",
       "1         0.005662  0.031445  0.114120  0.059877  0.041314  0.062247  0.058134  \n",
       "2         0.020806  0.045090  0.114118  0.057983  0.042641  0.050959  0.055869  \n",
       "3         0.021726  0.041143  0.115758  0.042198  0.046364  0.065316  0.077971  \n",
       "4         0.016970  0.057617  0.114235  0.049968  0.061272  0.051523  0.059396  \n",
       "...            ...       ...       ...       ...       ...       ...       ...  \n",
       "10999995  0.018622  0.028159  0.116204  0.065760  0.044546  0.050472  0.054919  \n",
       "10999996  0.018739  0.039215  0.119606  0.075558  0.047622  0.057437  0.066455  \n",
       "10999997  0.006538  0.032048  0.114361  0.053177  0.103900  0.076656  0.074661  \n",
       "10999998  0.019038  0.033915  0.141387  0.057126  0.016474  0.041420  0.048286  \n",
       "10999999  0.016887  0.026869  0.114274  0.037645  0.029818  0.021990  0.023210  \n",
       "\n",
       "[11000000 rows x 29 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f8de25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label=pd.read_csv(\"LabelSet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "233a4cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array(df_data.iloc[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26860715",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.array(df_label.iloc[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc6554a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=x.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "416810ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=x[:][:10000000]\n",
    "y_train=y[:][:10000000]\n",
    "x_test=x[:][10000000:]\n",
    "y_test=y[:][10000000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3de3c758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000000,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a84c77e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(X,no_nodes,Activation):#A custom Residual identity block with a skip connection\n",
    "    X_shortcut=X\n",
    "    #There will be 3 layers in total for the sequential connection\n",
    "    #Activation will be specified by the user as per the Keras Documentation\n",
    "    #no_nodes denote the size of the model which will be built\n",
    "    #Applying Batch Normalisation to size the inputs\n",
    "    #Layer 1 -----------------------------------------------\n",
    "    X=tf.keras.layers.Dense(no_nodes,activation=Activation)(X)\n",
    "    X=tf.keras.layers.BatchNormalization(axis=1)(X)\n",
    "    #--------------------------------------------------------\n",
    "    #Layer 2-------------------------------------------------\n",
    "    X=tf.keras.layers.Dense(no_nodes,activation=Activation)(X)\n",
    "    X=tf.keras.layers.BatchNormalization(axis=1)(X)\n",
    "    #--------------------------------------------------------\n",
    "    #Layer 3-------------------------------------------------\n",
    "    X=tf.keras.layers.Dense(no_nodes,activation=Activation)(X)\n",
    "    #--------------------------------------------------------\n",
    "    #Layer 4-------------------------------------------------\n",
    "    X= tf.keras.layers.Add()([X_shortcut, X])\n",
    "    X=tf.keras.layers.Activation('swish')(X)\n",
    "    #--------------------------------------------------------\n",
    "    X_shortcut2=X\n",
    "     #Layer 5 -----------------------------------------------\n",
    "    X=tf.keras.layers.Dense(no_nodes,activation=Activation)(X)\n",
    "    X=tf.keras.layers.BatchNormalization(axis=1)(X)\n",
    "    #--------------------------------------------------------\n",
    "    #Layer 6-------------------------------------------------\n",
    "    X=tf.keras.layers.Dense(no_nodes,activation=Activation)(X)\n",
    "    X=tf.keras.layers.BatchNormalization(axis=1)(X)\n",
    "    #--------------------------------------------------------\n",
    "    #Layer 7-------------------------------------------------\n",
    "    X=tf.keras.layers.Dense(no_nodes,activation=Activation)(X)\n",
    "    #--------------------------------------------------------\n",
    "    X= tf.keras.layers.Add()([X_shortcut, X,X_shortcut2])\n",
    "    #Layer 8-------------------------------------------------\n",
    "    X=tf.keras.layers.Activation('swish')(X)\n",
    "    return X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1dc516b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=tf.keras.layers.Input(shape=(28,))\n",
    "X=tf.keras.layers.Dense(300,activation='swish')(inputs)\n",
    "X=identity_block(X=X,no_nodes=300,Activation='swish')\n",
    "X=identity_block(X=X,no_nodes=300,Activation='swish')\n",
    "X=identity_block(X=X,no_nodes=300,Activation='swish')\n",
    "X=identity_block(X=X,no_nodes=300,Activation='swish')\n",
    "X=identity_block(X=X,no_nodes=300,Activation='swish')\n",
    "outputs=tf.keras.layers.Dense(1,activation='sigmoid')(X)\n",
    "model=tf.keras.Model(inputs,outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f5eb2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "        learning_rate=0.01,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy','AUC']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64c81210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x197e2d92f70>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "model.load_weights(latest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "398541ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  2/977 [..............................] - ETA: 3:00 - loss: 24.7014 - accuracy: 0.4857 - auc: 0.4970WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0790s vs `on_train_batch_end` time: 0.1390s). Check your callbacks.\n",
      "977/977 [==============================] - ETA: 0s - loss: 1.2460 - accuracy: 0.6078 - auc: 0.6403\n",
      "Epoch 00001: saving model to training_ResNet_52\\cp.ckpt\n",
      "977/977 [==============================] - 219s 224ms/step - loss: 1.2460 - accuracy: 0.6078 - auc: 0.6403 - val_loss: 0.6676 - val_accuracy: 0.6067 - val_auc: 0.6644\n",
      "Epoch 2/100\n",
      "977/977 [==============================] - ETA: 0s - loss: 0.5782 - accuracy: 0.6935 - auc: 0.7629\n",
      "Epoch 00002: saving model to training_ResNet_52\\cp.ckpt\n",
      "977/977 [==============================] - 226s 231ms/step - loss: 0.5782 - accuracy: 0.6935 - auc: 0.7629 - val_loss: 0.5903 - val_accuracy: 0.6832 - val_auc: 0.7535\n",
      "Epoch 3/100\n",
      "977/977 [==============================] - ETA: 0s - loss: 0.5429 - accuracy: 0.7203 - auc: 0.7969\n",
      "Epoch 00003: saving model to training_ResNet_52\\cp.ckpt\n",
      "977/977 [==============================] - 227s 232ms/step - loss: 0.5429 - accuracy: 0.7203 - auc: 0.7969 - val_loss: 0.5549 - val_accuracy: 0.7129 - val_auc: 0.7886\n",
      "Epoch 4/100\n",
      "977/977 [==============================] - ETA: 0s - loss: 0.5158 - accuracy: 0.7398 - auc: 0.8204\n",
      "Epoch 00004: saving model to training_ResNet_52\\cp.ckpt\n",
      "977/977 [==============================] - 222s 227ms/step - loss: 0.5158 - accuracy: 0.7398 - auc: 0.8204 - val_loss: 0.5187 - val_accuracy: 0.7379 - val_auc: 0.8186\n",
      "Epoch 5/100\n",
      "977/977 [==============================] - ETA: 0s - loss: 0.4933 - accuracy: 0.7549 - auc: 0.8378\n",
      "Epoch 00005: saving model to training_ResNet_52\\cp.ckpt\n",
      "977/977 [==============================] - 209s 214ms/step - loss: 0.4933 - accuracy: 0.7549 - auc: 0.8378 - val_loss: 0.5244 - val_accuracy: 0.7365 - val_auc: 0.8147\n",
      "Epoch 6/100\n",
      "977/977 [==============================] - ETA: 0s - loss: 0.4860 - accuracy: 0.7600 - auc: 0.8433\n",
      "Epoch 00006: saving model to training_ResNet_52\\cp.ckpt\n",
      "977/977 [==============================] - 213s 218ms/step - loss: 0.4860 - accuracy: 0.7600 - auc: 0.8433 - val_loss: 0.4943 - val_accuracy: 0.7545 - val_auc: 0.8371\n",
      "Epoch 7/100\n",
      "977/977 [==============================] - ETA: 0s - loss: 0.5318 - accuracy: 0.7340 - auc: 0.8159\n",
      "Epoch 00007: saving model to training_ResNet_52\\cp.ckpt\n",
      "977/977 [==============================] - 227s 232ms/step - loss: 0.5318 - accuracy: 0.7340 - auc: 0.8159 - val_loss: 0.6082 - val_accuracy: 0.6804 - val_auc: 0.7501\n",
      "Epoch 8/100\n",
      "977/977 [==============================] - ETA: 0s - loss: 0.5140 - accuracy: 0.7409 - auc: 0.8217\n",
      "Epoch 00008: saving model to training_ResNet_52\\cp.ckpt\n",
      "977/977 [==============================] - 226s 232ms/step - loss: 0.5140 - accuracy: 0.7409 - auc: 0.8217 - val_loss: 0.5254 - val_accuracy: 0.7385 - val_auc: 0.8210\n",
      "Epoch 9/100\n",
      "977/977 [==============================] - ETA: 0s - loss: 0.4873 - accuracy: 0.7586 - auc: 0.8421\n",
      "Epoch 00009: saving model to training_ResNet_52\\cp.ckpt\n",
      "977/977 [==============================] - 224s 229ms/step - loss: 0.4873 - accuracy: 0.7586 - auc: 0.8421 - val_loss: 0.4990 - val_accuracy: 0.7518 - val_auc: 0.8349\n",
      "Epoch 10/100\n",
      "977/977 [==============================] - ETA: 0s - loss: 0.4793 - accuracy: 0.7639 - auc: 0.8480\n",
      "Epoch 00010: saving model to training_ResNet_52\\cp.ckpt\n",
      "977/977 [==============================] - 216s 221ms/step - loss: 0.4793 - accuracy: 0.7639 - auc: 0.8480 - val_loss: 0.5016 - val_accuracy: 0.7500 - val_auc: 0.8350\n",
      "Epoch 11/100\n",
      "977/977 [==============================] - ETA: 0s - loss: 0.4724 - accuracy: 0.7684 - auc: 0.8529\n",
      "Epoch 00011: saving model to training_ResNet_52\\cp.ckpt\n",
      "977/977 [==============================] - 218s 223ms/step - loss: 0.4724 - accuracy: 0.7684 - auc: 0.8529 - val_loss: 0.4795 - val_accuracy: 0.7637 - val_auc: 0.8480\n",
      "Epoch 12/100\n",
      "977/977 [==============================] - ETA: 0s - loss: 0.4673 - accuracy: 0.7718 - auc: 0.8564\n",
      "Epoch 00012: saving model to training_ResNet_52\\cp.ckpt\n",
      "977/977 [==============================] - 226s 232ms/step - loss: 0.4673 - accuracy: 0.7718 - auc: 0.8564 - val_loss: 0.4725 - val_accuracy: 0.7686 - val_auc: 0.8534\n",
      "Epoch 13/100\n",
      "977/977 [==============================] - ETA: 0s - loss: 0.4636 - accuracy: 0.7743 - auc: 0.8590\n",
      "Epoch 00013: saving model to training_ResNet_52\\cp.ckpt\n",
      "977/977 [==============================] - 216s 221ms/step - loss: 0.4636 - accuracy: 0.7743 - auc: 0.8590 - val_loss: 0.4734 - val_accuracy: 0.7681 - val_auc: 0.8549\n",
      "Epoch 14/100\n",
      "977/977 [==============================] - ETA: 0s - loss: 0.4607 - accuracy: 0.7761 - auc: 0.8610\n",
      "Epoch 00014: saving model to training_ResNet_52\\cp.ckpt\n",
      "977/977 [==============================] - 224s 229ms/step - loss: 0.4607 - accuracy: 0.7761 - auc: 0.8610 - val_loss: 0.4750 - val_accuracy: 0.7675 - val_auc: 0.8521\n",
      "Epoch 15/100\n",
      "977/977 [==============================] - ETA: 0s - loss: 0.4581 - accuracy: 0.7776 - auc: 0.8627\n",
      "Epoch 00015: saving model to training_ResNet_52\\cp.ckpt\n",
      "977/977 [==============================] - 226s 232ms/step - loss: 0.4581 - accuracy: 0.7776 - auc: 0.8627 - val_loss: 0.4754 - val_accuracy: 0.7668 - val_auc: 0.8541\n",
      "Epoch 16/100\n",
      "977/977 [==============================] - ETA: 0s - loss: 0.6601 - accuracy: 0.6704 - auc: 0.7395\n",
      "Epoch 00016: saving model to training_ResNet_52\\cp.ckpt\n",
      "977/977 [==============================] - 226s 232ms/step - loss: 0.6601 - accuracy: 0.6704 - auc: 0.7395 - val_loss: 0.6337 - val_accuracy: 0.6300 - val_auc: 0.6847\n",
      "Epoch 17/100\n",
      "817/977 [========================>.....] - ETA: 35s - loss: 0.6074 - accuracy: 0.6648 - auc: 0.7266"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "2 root error(s) found.\n  (0) Resource exhausted:  OOM when allocating tensor with shape[10240,28] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node GatherV2}}]]\n\t [[IteratorGetNext]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted:  OOM when allocating tensor with shape[10240,28] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node GatherV2}}]]\n\t [[IteratorGetNext]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[assert_greater_equal/Assert/AssertGuard/else/_1/assert_greater_equal/Assert/AssertGuard/Assert/data_4/_116]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_11668]\n\nFunction call stack:\ntrain_function -> train_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-aaf7decf65e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m                                                  verbose=1)\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m model.fit(\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted:  OOM when allocating tensor with shape[10240,28] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node GatherV2}}]]\n\t [[IteratorGetNext]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted:  OOM when allocating tensor with shape[10240,28] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node GatherV2}}]]\n\t [[IteratorGetNext]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[assert_greater_equal/Assert/AssertGuard/else/_1/assert_greater_equal/Assert/AssertGuard/Assert/data_4/_116]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_11668]\n\nFunction call stack:\ntrain_function -> train_function\n"
     ]
    }
   ],
   "source": [
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    batch_size=10240,\n",
    "    validation_data=(x_test,y_test),\n",
    "    callbacks=[cp_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c1c918d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\cdipt\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\cdipt\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: ResNet_19\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('ResNet_52')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
