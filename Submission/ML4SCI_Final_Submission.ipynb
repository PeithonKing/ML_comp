{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "ML4SCI_Final_Submission.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "yvTrWnElCG_D",
        "E105mOT1AAfx",
        "bpivhS4qYU0g",
        "OANvbJjPYel5",
        "0mBN8KZNYuE9",
        "4tmCIzhfZtsQ",
        "weCqg1ZJZAhm",
        "k_o-BZKbZg4P",
        "LN_2eChrheWo",
        "hf41EezNj1rQ"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OZXTyXSvyeI"
      },
      "source": [
        "# Initialisation "
      ],
      "id": "1OZXTyXSvyeI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c2ae68a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fc99ed6-7e5e-40b8-d7fd-56995b4cf4b6"
      },
      "source": [
        "!pip install XGBoost==1.5.0"
      ],
      "id": "4c2ae68a",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: XGBoost==1.5.0 in /usr/local/lib/python3.7/dist-packages (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from XGBoost==1.5.0) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from XGBoost==1.5.0) (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6fNaxxfwSTe"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn import metrics\n",
        "import xgboost as xgb\n",
        "import tensorflow as tf\n",
        "from gc import collect\n",
        "import pandas as pd\n",
        "import os\n",
        "from time import time\n",
        "from gc import collect as delete"
      ],
      "id": "T6fNaxxfwSTe",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emP9exDg01fH"
      },
      "source": [
        "!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz\""
      ],
      "id": "emP9exDg01fH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqZuhSaQ2jfX"
      },
      "source": [
        "!wget --no-check-certificate -r \"https://docs.google.com/uc?export=download&id=1RlvdtcHKFu8RQ5Tv7IAhRDeGBW3QNwOH\" -O \"Weights.zip\""
      ],
      "id": "kqZuhSaQ2jfX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2XBqEn55BkP"
      },
      "source": [
        "!gzip -d HIGGS.csv.gz"
      ],
      "id": "o2XBqEn55BkP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1-gDsrM-6RE"
      },
      "source": [
        "!unzip Weights.zip"
      ],
      "id": "i1-gDsrM-6RE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvTrWnElCG_D"
      },
      "source": [
        "# Refining the data"
      ],
      "id": "yvTrWnElCG_D"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJ7KQtxOCLF2"
      },
      "source": [
        "def refine_data():\n",
        "    # Step 1: \n",
        "    df = pd.read_csv(\"HIGGS.csv\", header = None)\n",
        "\n",
        "    data_set = np.array(df.iloc[:, 1:])\n",
        "    label_set = np.array(df.iloc[:, 0])\n",
        "\n",
        "    # Getting rid of unnecessary data from RAM to\n",
        "    # accomodate within the 12GB RAM provided by Colab\n",
        "    del df\n",
        "    delete()\n",
        "\n",
        "    data_set = np.round(data_set, 5)\n",
        "\n",
        "    data_set = data_set.astype(np.float32)\n",
        "    label_set = label_set.astype(np.int32)\n",
        "\n",
        "    df_data_set = pd.DataFrame(data_set)\n",
        "    df_label_set = pd.DataFrame(label_set)\n",
        "\n",
        "    s = df_data_set.min()\n",
        "    l = df_data_set.max()\n",
        "\n",
        "    # Step 2: Normalization\n",
        "    normed_df_Data_set = ((df_data_set - s) / (l - s)).astype(np.float32)\n",
        "\n",
        "    # Saving data to file\n",
        "    df_label_set.to_csv(\"LabelSet.csv\")\n",
        "    normed_df_Data_set.to_csv(\"DataSetNormalised.csv\")"
      ],
      "id": "tJ7KQtxOCLF2",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E105mOT1AAfx"
      },
      "source": [
        "# All the Models\n",
        "## * These are the functions only.\n",
        "\n",
        "There are 6 models in our machine Learning Pipeline. We have put them in functions and the functions are here. We didn't call them here. they will be called later."
      ],
      "id": "E105mOT1AAfx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpivhS4qYU0g"
      },
      "source": [
        "#### Deep Neural Network DNN5 "
      ],
      "id": "bpivhS4qYU0g"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwXl5B51_7Py"
      },
      "source": [
        "def DNN5_fc():\n",
        "    '''Deep Neural Network DNN5'''\n",
        "    model=tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(300, input_shape=(28,), activation='swish'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dense(300, activation='swish'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dense(300, activation='swish'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dense(300, activation='swish'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dense(300, activation='swish'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(\n",
        "        optimizer = tf.keras.optimizers.Adam(\n",
        "        learning_rate = 0.01,\n",
        "        beta_1 = 0.9,\n",
        "        beta_2 = 0.999),\n",
        "        loss = 'binary_crossentropy',\n",
        "        metrics = ['accuracy','AUC']\n",
        "        )\n",
        "    model.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        epochs=1,\n",
        "        batch_size=10240,\n",
        "        validation_data=(x_test,y_test)\n",
        "    )\n",
        "    model.save('DNN_5')"
      ],
      "id": "CwXl5B51_7Py",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OANvbJjPYel5"
      },
      "source": [
        "#### Resnet 52 custom model"
      ],
      "id": "OANvbJjPYel5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1-5kxhYUlN9"
      },
      "source": [
        "def identity_block(X, no_nodes, Activation):\n",
        "    '''A custom Residual identity block with a skip connection'''\n",
        "    # This function is not to be called manually, it will be used by RN52_fc() function\n",
        "    X_shortcut=X\n",
        "    # There will be 3 layers in total for the sequential connection\n",
        "    # Activation will be specified by the user as per the Keras Documentation\n",
        "    # no_nodes denote the size of the model which will be built\n",
        "    # Applying Batch Normalisation to size the inputs\n",
        "    \n",
        "    #Layer 1\n",
        "    X=tf.keras.layers.Dense(no_nodes,activation=Activation)(X)\n",
        "    X=tf.keras.layers.BatchNormalization(axis=1)(X)\n",
        "    X=tf.keras.layers.Dropout(d_rate)(X)\n",
        "\n",
        "    #Layer 2\n",
        "    X=tf.keras.layers.Dense(no_nodes,activation=Activation)(X)\n",
        "    X=tf.keras.layers.BatchNormalization(axis=1)(X)\n",
        "    X=tf.keras.layers.Dropout(d_rate)(X)\n",
        "\n",
        "    #Layer 3\n",
        "    X=tf.keras.layers.Dense(no_nodes,activation=Activation)(X)\n",
        "    X=tf.keras.layers.Dropout(d_rate)(X)\n",
        "\n",
        "    #Layer 4\n",
        "    X= tf.keras.layers.Add()([X_shortcut, X])\n",
        "    X=tf.keras.layers.Activation('relu')(X)\n",
        "\n",
        "    X_shortcut2=X\n",
        "    #Layer 5\n",
        "    X=tf.keras.layers.Dense(no_nodes,activation=Activation)(X)\n",
        "    X=tf.keras.layers.BatchNormalization(axis=1)(X)\n",
        "    X=tf.keras.layers.Dropout(d_rate)(X)\n",
        "\n",
        "    #Layer 6\n",
        "    X=tf.keras.layers.Dense(no_nodes,activation=Activation)(X)\n",
        "    X=tf.keras.layers.BatchNormalization(axis=1)(X)\n",
        "    X=tf.keras.layers.Dropout(d_rate)(X)\n",
        "\n",
        "    #Layer 7\n",
        "    X=tf.keras.layers.Dense(no_nodes,activation=Activation)(X)\n",
        "    X=tf.keras.layers.Dropout(d_rate)(X)\n",
        "\n",
        "    #Layer 8\n",
        "    X = tf.keras.layers.Add()([X_shortcut, X,X_shortcut2])\n",
        "    X = tf.keras.layers.Activation('relu')(X)\n",
        "    return X\n",
        "\n",
        "def RN52_fc():\n",
        "    '''Resnet 52'''\n",
        "    d_rate=0.1 #The rate of droupout for the layers in the ResNet\n",
        "    act='gelu'#The activation function use ('relu','swish','elu','gelu','selu')\n",
        "    lr=0.01 #The default learning rate Default 0.01\n",
        "    mom=0.9 #The momentum parameter Default 0.9\n",
        "    test_split=1 #Default 1 and range[1,0)\n",
        "    #Reducing test_split will make the model faster but will also reduce the accuracy of the Validation metric\n",
        "    #On changing test_split run the lower initialisation cell, marked below**\n",
        "    \n",
        "    inputs=tf.keras.layers.Input(shape=(28,))                    #Keras input layer\n",
        "    X=tf.keras.layers.Dense(300,activation=act)(inputs)\n",
        "    X=identity_block(X=X,no_nodes=300,Activation=act)\n",
        "    X=identity_block(X=X,no_nodes=300,Activation=act)\n",
        "    X=identity_block(X=X,no_nodes=300,Activation=act)\n",
        "    X=identity_block(X=X,no_nodes=300,Activation=act)\n",
        "    X=identity_block(X=X,no_nodes=300,Activation=act)        \n",
        "    outputs=tf.keras.layers.Dense(1,activation='sigmoid')(X)     #keras output layer\n",
        "    model=tf.keras.Model(inputs,outputs)#Model declaration\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(\n",
        "        learning_rate=0.01,\n",
        "        beta_1=mom,\n",
        "        beta_2=0.999),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy','AUC']\n",
        "    )\n",
        "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\",\n",
        "                                                 factor=0.5,\n",
        "                                                 patience=5,\n",
        "                                                 min_lr=10**-6,\n",
        "                                                 cooldown=5,\n",
        "                                                 verbose = True)\n",
        "\n",
        "    history = History()\n",
        "\n",
        "    model.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        epochs=100,\n",
        "        batch_size=20000,\n",
        "        validation_data=(x_test,y_test),\n",
        "        callbacks=[cp_callback, reduce_lr, history]\n",
        "    )\n",
        "    model.save('ResNet_52')\n",
        "\n",
        "    with open(\"hstory.txt\", \"w\") as f:\n",
        "        f.write(str(history.history))"
      ],
      "id": "Q1-5kxhYUlN9",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mBN8KZNYuE9"
      },
      "source": [
        "#### XG Boost"
      ],
      "id": "0mBN8KZNYuE9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSi-6GY9CDu7"
      },
      "source": [
        "def XGB_fc():\n",
        "    '''XG Boost'''\n",
        "    model = XGBClassifier(tree_method='gpu_hist',\n",
        "                          booster = 'gbtree',\n",
        "                          max_depth = 14,\n",
        "                          n_estimators = 300,\n",
        "                          learning_rate = 0.4,\n",
        "                          reg_lambda=600\n",
        "                         )\n",
        "\n",
        "    eval_set = [(x_test_alpha, y_test_alpha)]\n",
        "\n",
        "    t0 = time()\n",
        "    a = model.fit(x_train_alpha,\n",
        "              y_train_alpha,\n",
        "              eval_metric=\"error\",\n",
        "              eval_set=eval_set,\n",
        "              verbose=True)\n",
        "    taken = time()-t0\n",
        "\n",
        "    y_test_alpha_pred = model.predict(x_test_alpha,ntree_limit=model.best_ntree_limit)\n",
        "    accuracy_test = accuracy_score(y_test_alpha, y_test_alpha_pred)\n",
        "    print(\"The testing accuracy\", accuracy_test*100)\n",
        "\n",
        "    y_train_alpha_pred=model.predict(x_train_alpha,ntree_limit=model.best_ntree_limit)\n",
        "    accuracy_train = accuracy_score(y_train_alpha, y_train_alpha_pred)\n",
        "    print(\"The training accuracy\", accuracy_train*100)\n",
        "    model.save_model(\"XGBoost.json\")"
      ],
      "id": "WSi-6GY9CDu7",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tmCIzhfZtsQ"
      },
      "source": [
        "#### Shallow Neural Network 1024 levels deep"
      ],
      "id": "4tmCIzhfZtsQ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29i-1qC7Zwov"
      },
      "source": [
        "def S1024_fc():\n",
        "    model=tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(1024,input_shape=(28,),activation='relu'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dense(1024,activation='relu'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dense(128,activation='swish'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dense(1,activation='sigmoid')\n",
        "        ])\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(\n",
        "        learning_rate=0.0001,\n",
        "        beta_1=0.9,\n",
        "        beta_2=0.999),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy','AUC']\n",
        "        )\n",
        "    \n",
        "    model.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        epochs=100,\n",
        "        batch_size=10240,\n",
        "        validation_data=(x_test,y_test),\n",
        "        callbacks=[cp_callback]\n",
        "        )\n",
        "    \n",
        "    model.save('NN_Shallow_1024')"
      ],
      "id": "29i-1qC7Zwov",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weCqg1ZJZAhm"
      },
      "source": [
        "#### Shallow Neural Network 2048 levels deep"
      ],
      "id": "weCqg1ZJZAhm"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5t4yg5y0ZNa-"
      },
      "source": [
        "def S1024_fc():\n",
        "    model=tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(1024,input_shape=(28,),activation='relu'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dense(1024,activation='relu'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dense(128,activation='swish'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dense(1,activation='sigmoid')\n",
        "        ])\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(\n",
        "        learning_rate=0.0001,\n",
        "        beta_1=0.9,\n",
        "        beta_2=0.999),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy','AUC']\n",
        "        )\n",
        "    \n",
        "    model.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        epochs=100,\n",
        "        batch_size=10240,\n",
        "        validation_data=(x_test,y_test),\n",
        "        callbacks=[cp_callback]\n",
        "        )\n",
        "    \n",
        "    model.save('NN_Shallow_1024')"
      ],
      "id": "5t4yg5y0ZNa-",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_o-BZKbZg4P"
      },
      "source": [
        "#### Shallow Neural Network 10240 levels deep"
      ],
      "id": "k_o-BZKbZg4P"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzDGjtfIZjPn"
      },
      "source": [
        "def S10240_fc():\n",
        "    model=tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10240,input_shape=(28,),activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(1,activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(\n",
        "                            learning_rate=0.001,\n",
        "                            beta_1=0.9,\n",
        "                            beta_2=0.999),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy','AUC']\n",
        "    )\n",
        "    \n",
        "    model.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        epochs=10,\n",
        "        batch_size=10240,\n",
        "        callbacks=[cp_callback]\n",
        "    )\n",
        "\n",
        "    model.save('NN_Shallow_2048')"
      ],
      "id": "ZzDGjtfIZjPn",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LN_2eChrheWo"
      },
      "source": [
        "# Ensemble (function only)"
      ],
      "id": "LN_2eChrheWo"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6nmA8ydhucf"
      },
      "source": [
        "def ens_fc(x_test):\n",
        "    \n",
        "    # Loading the models:\n",
        "    #    XG Boost\n",
        "    model_xgb = XGBClassifier()\n",
        "    model_xgb.load_model(\"XGBoost.json\")\n",
        "    y_test_pred_xgb = model_xgb.predict_proba(x_test)\n",
        "    y_pred_xgb = y_test_pred_xgb[:,1]\n",
        "    y_test_pred_xgb = np.reshape(y_pred_xgb,(1000000,1))\n",
        "    print(\"\\n\\nLoaded XGBoost\\n\\n\")\n",
        "\n",
        "    #    DNN 5\n",
        "    model_DNN_5 = tf.keras.models.load_model('DNN_5')\n",
        "    y_test_pred_DNN_5 = model_DNN_5.predict(x_test)\n",
        "    print(\"\\n\\nLoaded DNN 5\\n\\n\")\n",
        "\n",
        "    #    Shallow 1024\n",
        "    model_NN_Shallow_1024 = tf.keras.models.load_model('NN_Shallow_1024')\n",
        "    y_test_pred_NN_Shallow_1024 = model_NN_Shallow_1024.predict(x_test)\n",
        "    print(\"\\n\\nLoaded Shallow 1024\\n\\n\")\n",
        "\n",
        "    #    Shallow 2048\n",
        "    model_NN_Shallow_2048 = tf.keras.models.load_model('NN_Shallow_2048')\n",
        "    y_test_pred_NN_Shallow_2048 = model_NN_Shallow_2048.predict(x_test)\n",
        "    print(\"\\n\\nLoaded Shallow 2048\\n\\n\")\n",
        "\n",
        "    #    Shallow 10240\n",
        "    model_NN_Shallow_10240 = tf.keras.models.load_model('NN_Shallow_10240')\n",
        "    y_test_pred_NN_Shallow_10240 = model_NN_Shallow_10240.predict(x_test)\n",
        "    print(\"\\n\\nLoaded Shallow 10240\\n\\n\")\n",
        "\n",
        "    #    ResNet 52\n",
        "    model_ResNet_52 = tf.keras.models.load_model('ResNet_52')\n",
        "    y_test_pred_model_ResNet_52 = model_ResNet_52.predict(x_test)\n",
        "    print(\"\\n\\nLoaded ResNet 52\\n\\n\")\n",
        "\n",
        "    \n",
        "    # # The below code is used to find the fine-tuned values for ensembling all the models\n",
        "    # for i in range(10000):\n",
        "    #     pre=(np.random.rand(6,1))\n",
        "    #     y_pred_ensemble=y_test_pred_DNN_5*pre[0]\n",
        "    #     y_pred_ensemble=y_pred_ensemble+y_test_pred_model_ResNet_52*pre[1]\n",
        "    #     y_pred_ensemble=y_pred_ensemble+y_test_pred_NN_Shallow_1024*pre[2]\n",
        "    #     y_pred_ensemble=y_pred_ensemble+y_test_pred_NN_Shallow_10240*pre[3]\n",
        "    #     y_pred_ensemble=y_pred_ensemble+y_test_pred_NN_Shallow_2048*pre[4]\n",
        "    #     y_pred_ensemble=y_pred_ensemble+y_test_pred_xgb*pre[5]\n",
        "    #     y_pred_ensemble=y_pred_ensemble/(pre[0]+pre[1]+pre[2]+pre[3]+pre[4]+pre[5])\n",
        "    #     fpr, tpr, thresholds = roc_curve(y_test, y_pred_ensemble)\n",
        "    #     auc=metrics.auc(fpr, tpr)\n",
        "    #     if auc>auc_max:\n",
        "    #         auc_max=auc\n",
        "    #         print(\"New Max detected\")\n",
        "    #         print(\"Auc\",auc)\n",
        "    #         print(\"--------------------------------------------\")\n",
        "    #         pre_max=pre\n",
        "    #     if i%1000==0:\n",
        "    #         print(str(i))\n",
        "    \n",
        "    pre = (0.9874, 0.5109, 0.4046, 0.00178, 0.18467, 0.17939) # Calculated by finetuning\n",
        "\n",
        "    y_pred_ensemble = y_test_pred_DNN_5*pre[0]\n",
        "    print(\"Added XGBoost\")\n",
        "    y_pred_ensemble += y_test_pred_model_ResNet_52*pre[1]\n",
        "    print(\"Added DNN 5\")\n",
        "    y_pred_ensemble += y_test_pred_NN_Shallow_1024*pre[2]\n",
        "    print(\"Added Shallow 1024\")\n",
        "    y_pred_ensemble += y_test_pred_NN_Shallow_10240*pre[3]\n",
        "    print(\"Added Shallow 2048\")\n",
        "    y_pred_ensemble += y_test_pred_NN_Shallow_2048*pre[4]\n",
        "    print(\"Added Shallow 10240\")\n",
        "    y_pred_ensemble += y_test_pred_xgb*pre[5]\n",
        "    print(\"Added ResNet 52\")\n",
        "\n",
        "    y_pred_ensemble /= sum(pre)\n",
        "    # ntgt() # Used for fine tuning of model, @Aritra don't uncomment it\n",
        "    return y_pred_ensemble"
      ],
      "id": "O6nmA8ydhucf",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ni_q_rzHUOXN"
      },
      "source": [
        "# Calling the functions Sequentially"
      ],
      "id": "ni_q_rzHUOXN"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGe9CsoBJIYc"
      },
      "source": [
        "# Refining the data!\n",
        "refine_data()"
      ],
      "id": "KGe9CsoBJIYc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7_C4J-Yh8Aa"
      },
      "source": [
        "# Separating the data\n",
        "df_data = pd.read_csv(\"DataSetNormalised.csv\")\n",
        "df_label = pd.read_csv(\"LabelSet.csv\")\n",
        "x = np.array(df_data.iloc[:,1:])\n",
        "y = np.array(df_label.iloc[:,1])\n",
        "x = x.astype(np.float32)\n",
        "x_train = x[:][:1_00_00_000]\n",
        "y_train = y[:][:1_00_00_000]\n",
        "x_test = x[:][1_00_00_000:]\n",
        "y_test = y[:][1_00_00_000:]"
      ],
      "id": "A7_C4J-Yh8Aa",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zN1gDHEPgH-X"
      },
      "source": [
        "# # Calling the models for training\n",
        "# #       This has been intensionally commented out because\n",
        "# #       we have already trained them and downloaded the weights\n",
        "# #       from Google Drive. See Initialization part.\n",
        "# DNN5_fc()\n",
        "# RN52_fc()\n",
        "# XGB_fc()\n",
        "# S1024_fc()\n",
        "# S2048_fc()\n",
        "# S10240_fc()"
      ],
      "id": "zN1gDHEPgH-X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZpTPhWfioop",
        "outputId": "88eef460-60ac-4ab4-bfaf-4e97d28870b9"
      },
      "source": [
        "# Caling the ensemble\n",
        "# Send the data set X (After normalisation min-max)\n",
        "# keep y_test as the testing data set\n",
        "y_pred = ens_fc(x_test)"
      ],
      "id": "fZpTPhWfioop",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n",
            "WARNING:absl:Importing a function (__inference_sequential_2_layer_call_and_return_conditional_losses_202507) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_dense_15_layer_call_and_return_conditional_losses_203133) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_dense_14_layer_call_and_return_conditional_losses_203026) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_dense_13_layer_call_and_return_conditional_losses_201541) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference__wrapped_model_200754) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_dense_14_layer_call_and_return_conditional_losses_201608) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_dense_13_layer_call_and_return_conditional_losses_202919) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_dense_15_layer_call_and_return_conditional_losses_201675) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_dense_16_layer_call_and_return_conditional_losses_201742) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_dense_16_layer_call_and_return_conditional_losses_203240) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_dense_12_layer_call_and_return_conditional_losses_201474) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sequential_2_layer_call_and_return_conditional_losses_202658) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_dense_12_layer_call_and_return_conditional_losses_202812) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n",
            "WARNING:absl:Importing a function (__inference__wrapped_model_71612) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sequential_1_layer_call_and_return_conditional_losses_72687) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_dense_8_layer_call_and_return_conditional_losses_73082) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_sequential_1_layer_call_and_return_conditional_losses_72772) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_dense_8_layer_call_and_return_conditional_losses_72176) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hf41EezNj1rQ"
      },
      "source": [
        "# Finally Some Nice looking Curves"
      ],
      "id": "hf41EezNj1rQ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMsAZ7vEj5xo"
      },
      "source": [
        "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred)\n",
        "auc = metrics.roc_auc_score(y_test, y_pred)\n",
        "\n",
        "#create ROC curve\n",
        "plt.plot(fpr,tpr,label=\"AUC=\"+str(auc))\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "id": "kMsAZ7vEj5xo",
      "execution_count": null,
      "outputs": []
    }
  ]
}