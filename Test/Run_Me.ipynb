{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1a7cba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "#Entire Notebook \n",
    "#There are net 6 models in our ML PipeLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce5fac10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from gc import collect\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9ba142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"HIGGS.CSV\",header=None)\n",
    "data_set=np.array(df.iloc[:,1:])\n",
    "label_set=np.array(df.iloc[:,0])\n",
    "data_set=np.round(data_set,5)\n",
    "data_set=data_set.astype(np.float32)\n",
    "label_set=label_set.astype(np.int32)\n",
    "df_Data_set=pd.DataFrame(data_set)\n",
    "df_Label_set=pd.DataFrame(label_set)\n",
    "normed_df_Data_set = ((df_Data_set - df_Data_set.min()) / (df_Data_set.max() - df_Data_set.min())).astype(np.float32)\n",
    "normed_df_Data_set.to_csv(\"DataSetNormalised.csv\")\n",
    "df_Label_set.to_csv(\"LabelSet.csv\")\n",
    "#------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37b7effb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12561b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data=pd.read_csv(\"DataSetNormalised.csv\")\n",
    "df_label=pd.read_csv(\"LabelSet.csv\")\n",
    "x=np.array(df_data.iloc[:,1:])\n",
    "y=np.array(df_label.iloc[:,1])\n",
    "x=x.astype(np.float32)\n",
    "x_train=x[:][:10000000]\n",
    "y_train=y[:][:10000000]\n",
    "x_test=x[:][10000000:]\n",
    "y_test=y[:][10000000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ebfc181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.050286</td>\n",
       "      <td>0.369601</td>\n",
       "      <td>0.564642</td>\n",
       "      <td>0.021253</td>\n",
       "      <td>0.302234</td>\n",
       "      <td>0.062910</td>\n",
       "      <td>0.458154</td>\n",
       "      <td>0.186402</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.103508</td>\n",
       "      <td>...</td>\n",
       "      <td>0.497833</td>\n",
       "      <td>0.486773</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.031874</td>\n",
       "      <td>0.038707</td>\n",
       "      <td>0.113156</td>\n",
       "      <td>0.055765</td>\n",
       "      <td>0.038036</td>\n",
       "      <td>0.061924</td>\n",
       "      <td>0.067874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.053521</td>\n",
       "      <td>0.567601</td>\n",
       "      <td>0.603004</td>\n",
       "      <td>0.097277</td>\n",
       "      <td>0.410338</td>\n",
       "      <td>0.097729</td>\n",
       "      <td>0.406137</td>\n",
       "      <td>0.043934</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.054424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.271923</td>\n",
       "      <td>0.499667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005662</td>\n",
       "      <td>0.031445</td>\n",
       "      <td>0.114120</td>\n",
       "      <td>0.059877</td>\n",
       "      <td>0.041314</td>\n",
       "      <td>0.062247</td>\n",
       "      <td>0.058134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.044327</td>\n",
       "      <td>0.802000</td>\n",
       "      <td>0.030564</td>\n",
       "      <td>0.029457</td>\n",
       "      <td>0.622152</td>\n",
       "      <td>0.098682</td>\n",
       "      <td>0.715906</td>\n",
       "      <td>0.896692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.057842</td>\n",
       "      <td>...</td>\n",
       "      <td>0.725909</td>\n",
       "      <td>0.758205</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020806</td>\n",
       "      <td>0.045090</td>\n",
       "      <td>0.114118</td>\n",
       "      <td>0.057983</td>\n",
       "      <td>0.042641</td>\n",
       "      <td>0.050959</td>\n",
       "      <td>0.055869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.090465</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.768391</td>\n",
       "      <td>0.129367</td>\n",
       "      <td>0.753152</td>\n",
       "      <td>0.168172</td>\n",
       "      <td>0.222741</td>\n",
       "      <td>0.229380</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.194996</td>\n",
       "      <td>...</td>\n",
       "      <td>0.364120</td>\n",
       "      <td>0.109674</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021726</td>\n",
       "      <td>0.041143</td>\n",
       "      <td>0.115758</td>\n",
       "      <td>0.042198</td>\n",
       "      <td>0.046364</td>\n",
       "      <td>0.065316</td>\n",
       "      <td>0.077971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.070221</td>\n",
       "      <td>0.566001</td>\n",
       "      <td>0.936645</td>\n",
       "      <td>0.057322</td>\n",
       "      <td>0.154448</td>\n",
       "      <td>0.055491</td>\n",
       "      <td>0.319775</td>\n",
       "      <td>0.235269</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.053402</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425140</td>\n",
       "      <td>0.532329</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016970</td>\n",
       "      <td>0.057617</td>\n",
       "      <td>0.114235</td>\n",
       "      <td>0.049968</td>\n",
       "      <td>0.061272</td>\n",
       "      <td>0.051523</td>\n",
       "      <td>0.059396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999995</th>\n",
       "      <td>0.074864</td>\n",
       "      <td>0.708200</td>\n",
       "      <td>0.531054</td>\n",
       "      <td>0.097118</td>\n",
       "      <td>0.345951</td>\n",
       "      <td>0.224923</td>\n",
       "      <td>0.358620</td>\n",
       "      <td>0.879183</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.145714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.480493</td>\n",
       "      <td>0.841457</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.018622</td>\n",
       "      <td>0.028159</td>\n",
       "      <td>0.116204</td>\n",
       "      <td>0.065760</td>\n",
       "      <td>0.044546</td>\n",
       "      <td>0.050472</td>\n",
       "      <td>0.054919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999996</th>\n",
       "      <td>0.029067</td>\n",
       "      <td>0.292001</td>\n",
       "      <td>0.818375</td>\n",
       "      <td>0.061103</td>\n",
       "      <td>0.391357</td>\n",
       "      <td>0.088460</td>\n",
       "      <td>0.558688</td>\n",
       "      <td>0.017989</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.071006</td>\n",
       "      <td>...</td>\n",
       "      <td>0.456486</td>\n",
       "      <td>0.800867</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.018739</td>\n",
       "      <td>0.039215</td>\n",
       "      <td>0.119606</td>\n",
       "      <td>0.075558</td>\n",
       "      <td>0.047622</td>\n",
       "      <td>0.057437</td>\n",
       "      <td>0.066455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999997</th>\n",
       "      <td>0.036016</td>\n",
       "      <td>0.659000</td>\n",
       "      <td>0.936008</td>\n",
       "      <td>0.055004</td>\n",
       "      <td>0.560670</td>\n",
       "      <td>0.097729</td>\n",
       "      <td>0.508838</td>\n",
       "      <td>0.507019</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.100894</td>\n",
       "      <td>...</td>\n",
       "      <td>0.817271</td>\n",
       "      <td>0.991563</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006538</td>\n",
       "      <td>0.032048</td>\n",
       "      <td>0.114361</td>\n",
       "      <td>0.053177</td>\n",
       "      <td>0.103900</td>\n",
       "      <td>0.076656</td>\n",
       "      <td>0.074661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999998</th>\n",
       "      <td>0.076397</td>\n",
       "      <td>0.524201</td>\n",
       "      <td>0.133552</td>\n",
       "      <td>0.121080</td>\n",
       "      <td>0.332522</td>\n",
       "      <td>0.087833</td>\n",
       "      <td>0.287096</td>\n",
       "      <td>0.866448</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047491</td>\n",
       "      <td>...</td>\n",
       "      <td>0.780094</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019038</td>\n",
       "      <td>0.033915</td>\n",
       "      <td>0.141387</td>\n",
       "      <td>0.057126</td>\n",
       "      <td>0.016474</td>\n",
       "      <td>0.041420</td>\n",
       "      <td>0.048286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999999</th>\n",
       "      <td>0.016050</td>\n",
       "      <td>0.430800</td>\n",
       "      <td>0.565597</td>\n",
       "      <td>0.061985</td>\n",
       "      <td>0.251052</td>\n",
       "      <td>0.029838</td>\n",
       "      <td>0.454319</td>\n",
       "      <td>0.140397</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.022724</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169056</td>\n",
       "      <td>0.331733</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016887</td>\n",
       "      <td>0.026869</td>\n",
       "      <td>0.114274</td>\n",
       "      <td>0.037645</td>\n",
       "      <td>0.029818</td>\n",
       "      <td>0.021990</td>\n",
       "      <td>0.023210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11000000 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1         2         3         4         5   \\\n",
       "0         0.050286  0.369601  0.564642  0.021253  0.302234  0.062910   \n",
       "1         0.053521  0.567601  0.603004  0.097277  0.410338  0.097729   \n",
       "2         0.044327  0.802000  0.030564  0.029457  0.622152  0.098682   \n",
       "3         0.090465  0.320000  0.768391  0.129367  0.753152  0.168172   \n",
       "4         0.070221  0.566001  0.936645  0.057322  0.154448  0.055491   \n",
       "...            ...       ...       ...       ...       ...       ...   \n",
       "10999995  0.074864  0.708200  0.531054  0.097118  0.345951  0.224923   \n",
       "10999996  0.029067  0.292001  0.818375  0.061103  0.391357  0.088460   \n",
       "10999997  0.036016  0.659000  0.936008  0.055004  0.560670  0.097729   \n",
       "10999998  0.076397  0.524201  0.133552  0.121080  0.332522  0.087833   \n",
       "10999999  0.016050  0.430800  0.565597  0.061985  0.251052  0.029838   \n",
       "\n",
       "                6         7    8         9   ...        18        19   20  \\\n",
       "0         0.458154  0.186402  0.0  0.103508  ...  0.497833  0.486773  1.0   \n",
       "1         0.406137  0.043934  1.0  0.054424  ...  0.271923  0.499667  0.0   \n",
       "2         0.715906  0.896692  0.0  0.057842  ...  0.725909  0.758205  0.0   \n",
       "3         0.222741  0.229380  0.0  0.194996  ...  0.364120  0.109674  0.0   \n",
       "4         0.319775  0.235269  0.0  0.053402  ...  0.425140  0.532329  0.0   \n",
       "...            ...       ...  ...       ...  ...       ...       ...  ...   \n",
       "10999995  0.358620  0.879183  0.0  0.145714  ...  0.480493  0.841457  1.0   \n",
       "10999996  0.558688  0.017989  1.0  0.071006  ...  0.456486  0.800867  1.0   \n",
       "10999997  0.508838  0.507019  1.0  0.100894  ...  0.817271  0.991563  0.0   \n",
       "10999998  0.287096  0.866448  0.0  0.047491  ...  0.780094  0.123205  0.0   \n",
       "10999999  0.454319  0.140397  1.0  0.022724  ...  0.169056  0.331733  0.0   \n",
       "\n",
       "                21        22        23        24        25        26        27  \n",
       "0         0.031874  0.038707  0.113156  0.055765  0.038036  0.061924  0.067874  \n",
       "1         0.005662  0.031445  0.114120  0.059877  0.041314  0.062247  0.058134  \n",
       "2         0.020806  0.045090  0.114118  0.057983  0.042641  0.050959  0.055869  \n",
       "3         0.021726  0.041143  0.115758  0.042198  0.046364  0.065316  0.077971  \n",
       "4         0.016970  0.057617  0.114235  0.049968  0.061272  0.051523  0.059396  \n",
       "...            ...       ...       ...       ...       ...       ...       ...  \n",
       "10999995  0.018622  0.028159  0.116204  0.065760  0.044546  0.050472  0.054919  \n",
       "10999996  0.018739  0.039215  0.119606  0.075558  0.047622  0.057437  0.066455  \n",
       "10999997  0.006538  0.032048  0.114361  0.053177  0.103900  0.076656  0.074661  \n",
       "10999998  0.019038  0.033915  0.141387  0.057126  0.016474  0.041420  0.048286  \n",
       "10999999  0.016887  0.026869  0.114274  0.037645  0.029818  0.021990  0.023210  \n",
       "\n",
       "[11000000 rows x 28 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normed_df_Data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3eb46241",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "   \n",
    "#--------------------------------------------------------------\n",
    "def DNN5_fc():\n",
    "    \n",
    "#Deep Neural Network DNN5\n",
    "    model=tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(300,input_shape=(28,),activation='swish'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(300,activation='swish'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(300,activation='swish'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(300,activation='swish'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(300,activation='swish'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "        learning_rate=0.01,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy','AUC']\n",
    "        )\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        epochs=1,\n",
    "        batch_size=10240,\n",
    "        validation_data=(x_test,y_test)\n",
    "    )\n",
    "    model.save('DNN_5')\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "def RN52_fc():\n",
    "    d_rate=0.1 #The rate of droupout for the layers in the ResNet\n",
    "    act='gelu'#The activation function use ('relu','swish','elu','gelu','selu')\n",
    "    lr=0.01 #The default learning rate Default 0.01\n",
    "    mom=0.9 #The momentum parameter Default 0.9\n",
    "    test_split=1 #Default 1 and range[1,0)\n",
    "    #Reducing test_split will make the model faster but will also reduce the accuracy of the Validation metric\n",
    "    #On changing test_split run the lower initialisation cell, marked below**\n",
    "    \n",
    "    def identity_block(X,no_nodes,Activation):#A custom Residual identity block with a skip connection\n",
    "        X_shortcut=X\n",
    "        #There will be 3 layers in total for the sequential connection\n",
    "        #Activation will be specified by the user as per the Keras Documentation\n",
    "        #no_nodes denote the size of the model which will be built\n",
    "        #Applying Batch Normalisation to size the inputs\n",
    "        #Layer 1 -----------------------------------------------\n",
    "        X=tf.keras.layers.Dense(no_nodes,activation=Activation)(X)\n",
    "        X=tf.keras.layers.BatchNormalization(axis=1)(X)\n",
    "        X=tf.keras.layers.Dropout(d_rate)(X)\n",
    "        #--------------------------------------------------------\n",
    "        #Layer 2-------------------------------------------------\n",
    "        X=tf.keras.layers.Dense(no_nodes,activation=Activation)(X)\n",
    "        X=tf.keras.layers.BatchNormalization(axis=1)(X)\n",
    "        X=tf.keras.layers.Dropout(d_rate)(X)\n",
    "        #--------------------------------------------------------\n",
    "        #Layer 3-------------------------------------------------\n",
    "        X=tf.keras.layers.Dense(no_nodes,activation=Activation)(X)\n",
    "        X=tf.keras.layers.Dropout(d_rate)(X)\n",
    "        #--------------------------------------------------------\n",
    "        #Layer 4-------------------------------------------------\n",
    "        X= tf.keras.layers.Add()([X_shortcut, X])\n",
    "        X=tf.keras.layers.Activation('relu')(X)\n",
    "        #--------------------------------------------------------\n",
    "        X_shortcut2=X\n",
    "         #Layer 5 -----------------------------------------------\n",
    "        X=tf.keras.layers.Dense(no_nodes,activation=Activation)(X)\n",
    "        X=tf.keras.layers.BatchNormalization(axis=1)(X)\n",
    "        X=tf.keras.layers.Dropout(d_rate)(X)\n",
    "        #--------------------------------------------------------\n",
    "        #Layer 6-------------------------------------------------\n",
    "        X=tf.keras.layers.Dense(no_nodes,activation=Activation)(X)\n",
    "        X=tf.keras.layers.BatchNormalization(axis=1)(X)\n",
    "        X=tf.keras.layers.Dropout(d_rate)(X)\n",
    "        #--------------------------------------------------------\n",
    "        #Layer 7-------------------------------------------------\n",
    "        X=tf.keras.layers.Dense(no_nodes,activation=Activation)(X)\n",
    "        X=tf.keras.layers.Dropout(d_rate)(X)\n",
    "        #--------------------------------------------------------\n",
    "        X= tf.keras.layers.Add()([X_shortcut, X,X_shortcut2])\n",
    "        #Layer 8-------------------------------------------------\n",
    "        X=tf.keras.layers.Activation('relu')(X)\n",
    "        return X\n",
    "    inputs=tf.keras.layers.Input(shape=(28,))                    #Keras input layer\n",
    "    X=tf.keras.layers.Dense(300,activation=act)(inputs)\n",
    "    X=identity_block(X=X,no_nodes=300,Activation=act)\n",
    "    X=identity_block(X=X,no_nodes=300,Activation=act)\n",
    "    X=identity_block(X=X,no_nodes=300,Activation=act)\n",
    "    X=identity_block(X=X,no_nodes=300,Activation=act)\n",
    "    X=identity_block(X=X,no_nodes=300,Activation=act)        \n",
    "    outputs=tf.keras.layers.Dense(1,activation='sigmoid')(X)     #keras output layer\n",
    "    model=tf.keras.Model(inputs,outputs)#Model declaration\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "        learning_rate=0.01,\n",
    "        beta_1=mom,\n",
    "        beta_2=0.999),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy','AUC']\n",
    "    )\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\",\n",
    "                                                 factor=0.5,\n",
    "                                                 patience=5,\n",
    "                                                 min_lr=10**-6,\n",
    "                                                 cooldown=5,\n",
    "                                                 verbose = True)\n",
    "\n",
    "    history = History()\n",
    "\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        epochs=100,\n",
    "        batch_size=20000,\n",
    "        validation_data=(x_test,y_test),\n",
    "        callbacks=[cp_callback, reduce_lr, history]\n",
    "    )\n",
    "    model.save('ResNet_52')\n",
    "\n",
    "    with open(\"sunnyMamoni.txt\", \"w\") as f:\n",
    "        f.write(str(history.history))\n",
    "#-----------------------------------------------------------------------------------    \n",
    "def XGB_fc():\n",
    "        \n",
    "    model = XGBClassifier(tree_method='gpu_hist',\n",
    "                          booster = 'gbtree',\n",
    "                          max_depth = 14,\n",
    "                          n_estimators = 300,\n",
    "                          learning_rate = 0.4,\n",
    "                          reg_lambda=600\n",
    "                         ) # booster = gbtree, dart\n",
    "\n",
    "    eval_set = [(x_test_alpha, y_test_alpha)]\n",
    "\n",
    "    t0 = time()\n",
    "    a = model.fit(x_train_alpha,\n",
    "              y_train_alpha,\n",
    "              eval_metric=\"error\",\n",
    "              eval_set=eval_set,\n",
    "              verbose=True)\n",
    "    taken = time()-t0\n",
    "\n",
    "    y_test_alpha_pred = model.predict(x_test_alpha,ntree_limit=model.best_ntree_limit)\n",
    "    accuracy_test = accuracy_score(y_test_alpha, y_test_alpha_pred)\n",
    "    print(\"The testing accuracy\", accuracy_test*100)\n",
    "\n",
    "    y_train_alpha_pred=model.predict(x_train_alpha,ntree_limit=model.best_ntree_limit)\n",
    "    accuracy_train = accuracy_score(y_train_alpha, y_train_alpha_pred)\n",
    "    print(\"The training accuracy\", accuracy_train*100)\n",
    "    model.save_model(\"XGBoost.json\")\n",
    "#------------------------------------------------------\n",
    "def S1024_fc():\n",
    "\n",
    "    model=tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(1024,input_shape=(28,),activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(1024,activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(128,activation='swish'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "        ])\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "        learning_rate=0.0001,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy','AUC']\n",
    "        )\n",
    "    model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    batch_size=10240,\n",
    "    validation_data=(x_test,y_test),\n",
    "    callbacks=[cp_callback]\n",
    "        )\n",
    "    model.save('NN_Shallow_1024')\n",
    "#-------------------------------------------------------------------\n",
    "def S2048_fc():\n",
    "    model=tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(2048,input_shape=(28,),activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(2048,activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "        ])\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "        learning_rate=0.01,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy','AUC']\n",
    "        )\n",
    "    model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=1,\n",
    "    batch_size=10240,\n",
    "    validation_data=(x_test,y_test),\n",
    "    callbacks=[cp_callback]\n",
    "        )\n",
    "    model.save('NN_Shallow_2048')\n",
    "#---------------------------------------------------------------------------\n",
    "def S10240_fc():\n",
    "    model=tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10240,input_shape=(28,),activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(\n",
    "        learning_rate=0.001,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy','AUC']\n",
    "    )\n",
    "    model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    batch_size=10240,\n",
    "    callbacks=[cp_callback]\n",
    "    )\n",
    "    model.save('NN_Shallow_2048')\n",
    "#----------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e177355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ens_fc(x_test):    \n",
    "    model_xgb=XGBClassifier()\n",
    "    model_xgb.load_model(\"XGBoost.json\")\n",
    "    y_test_pred_xgb=model_xgb.predict_proba(x_test)\n",
    "    y_pred_xgb=y_test_pred_xgb[:,1]\n",
    "    y_test_pred_xgb=np.reshape(y_pred_xgb,(1000000,1))\n",
    "    model_DNN_5 = tf.keras.models.load_model('DNN_5')\n",
    "    y_test_pred_DNN_5=model_DNN_5.predict(x_test)\n",
    "    model_NN_Shallow_1024 = tf.keras.models.load_model('NN_Shallow_1024')\n",
    "    y_test_pred_NN_Shallow_1024=model_NN_Shallow_1024.predict(x_test)\n",
    "    model_NN_Shallow_2048 = tf.keras.models.load_model('NN_Shallow_2048')\n",
    "    y_test_pred_NN_Shallow_2048=model_NN_Shallow_2048.predict(x_test)\n",
    "    model_NN_Shallow_10240 = tf.keras.models.load_model('NN_Shallow_10240')\n",
    "    y_test_pred_NN_Shallow_10240=model_NN_Shallow_10240.predict(x_test)\n",
    "    model_ResNet_52 = tf.keras.models.load_model('ResNet_52')\n",
    "    y_test_pred_model_ResNet_52=model_ResNet_52.predict(x_test)\n",
    "\n",
    "    \"\"\"\n",
    "    for i in range(10000):\n",
    "    pre=(np.random.rand(6,1))\n",
    "    y_pred_ensemble=y_test_pred_DNN_5*pre[0]\n",
    "    y_pred_ensemble=y_pred_ensemble+y_test_pred_model_ResNet_52*pre[1]\n",
    "    y_pred_ensemble=y_pred_ensemble+y_test_pred_NN_Shallow_1024*pre[2]\n",
    "    y_pred_ensemble=y_pred_ensemble+y_test_pred_NN_Shallow_10240*pre[3]\n",
    "    y_pred_ensemble=y_pred_ensemble+y_test_pred_NN_Shallow_2048*pre[4]\n",
    "    y_pred_ensemble=y_pred_ensemble+y_test_pred_xgb*pre[5]\n",
    "    y_pred_ensemble=y_pred_ensemble/(pre[0]+pre[1]+pre[2]+pre[3]+pre[4]+pre[5])\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_ensemble)\n",
    "    auc=metrics.auc(fpr, tpr)\n",
    "    if auc>auc_max:\n",
    "        auc_max=auc\n",
    "        print(\"New Max detected\")\n",
    "        print(\"Auc\",auc)\n",
    "        print(\"--------------------------------------------\")\n",
    "        pre_max=pre\n",
    "    if i%1000==0:\n",
    "        print(str(i))\n",
    "    The above code is used to find the fine-tuned values for ensembling all the models\n",
    "    \"\"\"\n",
    "    pre=(0.9874,0.5109,0.4046,0.00178,0.18467,0.17939)\n",
    "    y_pred_ensemble=y_test_pred_DNN_5*pre[0]\n",
    "    y_pred_ensemble=y_pred_ensemble+y_test_pred_model_ResNet_52*pre[1]\n",
    "    y_pred_ensemble=y_pred_ensemble+y_test_pred_NN_Shallow_1024*pre[2]\n",
    "    y_pred_ensemble=y_pred_ensemble+y_test_pred_NN_Shallow_10240*pre[3]\n",
    "    y_pred_ensemble=y_pred_ensemble+y_test_pred_NN_Shallow_2048*pre[4]\n",
    "    y_pred_ensemble=y_pred_ensemble+y_test_pred_xgb*pre[5]\n",
    "    y_pred_ensemble=y_pred_ensemble/(pre[0]+pre[1]+pre[2]+pre[3]+pre[4]+pre[5])\n",
    "    #ntgt() Used for fine tuning of model, please dont uncomment it\n",
    "    return y_pred_ensemble    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a739717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_sequential_2_layer_call_and_return_conditional_losses_202507) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_dense_15_layer_call_and_return_conditional_losses_203133) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_dense_14_layer_call_and_return_conditional_losses_203026) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_dense_13_layer_call_and_return_conditional_losses_201541) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference__wrapped_model_200754) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_dense_14_layer_call_and_return_conditional_losses_201608) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_dense_13_layer_call_and_return_conditional_losses_202919) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_dense_15_layer_call_and_return_conditional_losses_201675) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_dense_16_layer_call_and_return_conditional_losses_201742) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_dense_16_layer_call_and_return_conditional_losses_203240) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_dense_12_layer_call_and_return_conditional_losses_201474) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sequential_2_layer_call_and_return_conditional_losses_202658) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_dense_12_layer_call_and_return_conditional_losses_202812) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n",
      "WARNING:absl:Importing a function (__inference__wrapped_model_71612) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sequential_1_layer_call_and_return_conditional_losses_72687) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_dense_8_layer_call_and_return_conditional_losses_73082) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sequential_1_layer_call_and_return_conditional_losses_72772) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_dense_8_layer_call_and_return_conditional_losses_72176) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
     ]
    }
   ],
   "source": [
    "y_pred=ens_fc(x_test) #Send the data set X(After normalisation min-max)\n",
    "#keep y_test as the testing data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "091260ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "\n",
    "#create ROC curve\n",
    "plt.plot(fpr,tpr,label=\"AUC=\"+str(auc))\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
